using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using RE.Core.Interfaces;
using RE.Core.Interfaces.Services;
using RE.Core.Models.Scrapper;

namespace RE.Infrastructure.Services
{
    public class ScrapperService : IScrapperService
    {
        private readonly IApplicationDbContext _dbContext;
        private readonly IModuleService _moduleService;
        private readonly ILogger<ScrapperService> _logger;

        public ScrapperService(
            IApplicationDbContext dbContext,
            IModuleService moduleService,
            ILogger<ScrapperService> logger)
        {
            _dbContext = dbContext;
            _moduleService = moduleService;
            _logger = logger;
        }

        public async Task<IList<Ad>> ScrapOffers(CancellationToken cancellationToken)
        {
            var scrappers = await _moduleService.GetScrappingTypeModules(cancellationToken).ToListAsync(cancellationToken);

            _logger.LogInformation($"\n{DateTime.Now:HH:mm:ss} - Number of scrappers to check: {scrappers.Count}");

            var aggregatedOffers = new List<Ad>();
            foreach (var scrapper in scrappers.Where(IsActive).OrderBy(PriorityOrder))
            {
                var offers = Scrap(scrapper);
                aggregatedOffers.AddRange(offers);
            }

            return aggregatedOffers;
        }

        public async Task<IList<Ad>> ScrapOffers(IList<Uri> offerUris, CancellationToken cancellationToken)
        {
            var scrappers = await _moduleService.GetScrappingTypeModules(cancellationToken).ToListAsync(cancellationToken);

            var aggregatedOffers = new List<Ad>();
            var scrapperGroups = from offerUri in offerUris group offerUri by offerUri.Host into offers
                                 join website in _dbContext.Websites on offers.Key equals website.Domain 
                                 join scrapper in scrappers on website.Scrapper equals scrapper.GetType().AssemblyQualifiedName 
                                 select new { scrapper, offerUris = offers.ToList() };
            foreach (var scrapperGroup in scrapperGroups.OrderBy(sg => PriorityOrder(sg.scrapper)))
            {
                var offers = Scrap(scrapperGroup.scrapper, scrapperGroup.offerUris);
                aggregatedOffers.AddRange(offers);
            }

            return aggregatedOffers;
        }

        private bool IsActive(ScrapperModule scrapper)
        {
           var website = _dbContext.Websites.Single(w => w.Scrapper == scrapper.GetType().AssemblyQualifiedName);
           return website.Active;
        }

        private short PriorityOrder(ScrapperModule scrapper)
        {
            var website = _dbContext.Websites.Single(w => w.Scrapper == scrapper.GetType().AssemblyQualifiedName);
            return website.Priority;
        }

        private IList<Ad> Scrap(ScrapperModule scrapper)
        {
            var majorSw = new Stopwatch();
            majorSw.Start();

            var offers = scrapper.ParseOffers(majorSw);
            _logger.LogInformation($"{scrapper.Name} finished, (elapsed time : {majorSw.Elapsed.Minutes}:{majorSw.Elapsed.Seconds}:{majorSw.Elapsed.Milliseconds} [min:sec:ms])", true);

            return offers;
        }

        private IList<Ad> Scrap(ScrapperModule scrapper, IList<Uri> offerUris)
        {
            var majorSw = new Stopwatch();
            majorSw.Start();

            _logger.LogInformation($"{DateTime.Now:HH:mm:ss} - {scrapper.Name} scrapper started!");
            var offers = scrapper.ParseOffers(majorSw, offerUris.ToList());
            _logger.LogInformation($"{scrapper.Name} finished, (elapsed time : {majorSw.Elapsed.Minutes}:{majorSw.Elapsed.Seconds}:{majorSw.Elapsed.Milliseconds} [min:sec:ms])", true);

            return offers;
        }
    }
}
